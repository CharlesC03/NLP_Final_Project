{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772e63c4",
   "metadata": {},
   "source": [
    "# Preparing Data for Distallation\n",
    "\n",
    "Charles Ciampa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable\n",
    "import warnings\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Added for notifier import\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from notifier import Notifier\n",
    "\n",
    "NOTIFIER = Notifier(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd6f05c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ceciampa/code/NLP_Final_Project/.pixi/envs/default/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'notebook_login': pass new_session=False as keyword args. From version 1.0 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8ecd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFCacheInfo(size_on_disk=32407515309, repos=frozenset({CachedRepoInfo(repo_id='peixian/equity_evaluation_corpus', repo_type='dataset', repo_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/datasets--peixian--equity_evaluation_corpus'), size_on_disk=8937, nb_files=2, revisions=frozenset({CachedRevisionInfo(commit_hash='0f68047bb0d5d17e273ea7bd87b8964cdbe00028', snapshot_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/datasets--peixian--equity_evaluation_corpus/snapshots/0f68047bb0d5d17e273ea7bd87b8964cdbe00028'), size_on_disk=8937, files=frozenset({CachedFileInfo(file_name='equity_evaluation_corpus.py', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/datasets--peixian--equity_evaluation_corpus/snapshots/0f68047bb0d5d17e273ea7bd87b8964cdbe00028/equity_evaluation_corpus.py'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/datasets--peixian--equity_evaluation_corpus/blobs/59af1147e6237ef0e94bd042259ef9319334917f'), size_on_disk=5163, blob_last_accessed=1764551485.7068753, blob_last_modified=1764551485.767864), CachedFileInfo(file_name='README.md', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/datasets--peixian--equity_evaluation_corpus/snapshots/0f68047bb0d5d17e273ea7bd87b8964cdbe00028/README.md'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/datasets--peixian--equity_evaluation_corpus/blobs/796a74e967b110aedc934c435ab9dd5d5f089abc'), size_on_disk=3774, blob_last_accessed=1764551485.442591, blob_last_modified=1764551485.574733)}), refs=frozenset({'main'}), last_modified=1764551485.767864)}), last_accessed=1764551485.7068753, last_modified=1764551485.767864), CachedRepoInfo(repo_id='distilbert/distilbert-base-uncased-finetuned-sst-2-english', repo_type='model', repo_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english'), size_on_disk=268064743, nb_files=4, revisions=frozenset({CachedRevisionInfo(commit_hash='714eb0fa89d2f80546fda750413ed43d93601a13', snapshot_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13'), size_on_disk=268064743, files=frozenset({CachedFileInfo(file_name='model.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/model.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/blobs/7c3919835e442510166d267fe7cbe847e0c51cd26d9ba07b89a57b952b49b8aa'), size_on_disk=267832558, blob_last_accessed=1763073138.6809552, blob_last_modified=1763073138.6401477), CachedFileInfo(file_name='config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/blobs/b57fe5dfcb8ec3f9bab35ed427c3434e3c7dd1ba'), size_on_disk=629, blob_last_accessed=1763073135.9585092, blob_last_modified=1763073135.938044), CachedFileInfo(file_name='tokenizer_config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/tokenizer_config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/blobs/3ed34255a7cb8e6706a8bb21993836e99e7b959f'), size_on_disk=48, blob_last_accessed=1763073139.2726648, blob_last_modified=1763073138.793176), CachedFileInfo(file_name='vocab.txt', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/vocab.txt'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938'), size_on_disk=231508, blob_last_accessed=1763073139.2726648, blob_last_modified=1763073138.987012)}), refs=frozenset({'714eb0f'}), last_modified=1763073138.987012)}), last_accessed=1763073139.2726648, last_modified=1763073138.987012), CachedRepoInfo(repo_id='meta-llama/Meta-Llama-3.1-8B-Instruct', repo_type='model', repo_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct'), size_on_disk=16069722669, nb_files=10, revisions=frozenset({CachedRevisionInfo(commit_hash='0e9e39f249a16976918f6564b8830bc894c89659', snapshot_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'), size_on_disk=16069722669, files=frozenset({CachedFileInfo(file_name='model.safetensors.index.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model.safetensors.index.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/0fd8120f1c6acddc268ebc2583058efaf699a771'), size_on_disk=23950, blob_last_accessed=1764885820.690037, blob_last_modified=1763094181.320526), CachedFileInfo(file_name='config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/0bb6fd75b3ad2fe988565929f329945262c2814e'), size_on_disk=855, blob_last_accessed=1764885820.4962554, blob_last_modified=1763094181.0281768), CachedFileInfo(file_name='model-00003-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00003-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/fc1cdddd6bfa91128d6e94ee73d0ce62bfcdb7af29e978ddcab30c66ae9ea7fa'), size_on_disk=4915916176, blob_last_accessed=1764885830.5410206, blob_last_modified=1763094406.1886187), CachedFileInfo(file_name='generation_config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/generation_config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/cc7276afd599de091142c6ed3005faf8a74aa257'), size_on_disk=184, blob_last_accessed=1764885835.081519, blob_last_modified=1763094412.2045398), CachedFileInfo(file_name='model-00001-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00001-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/2b1879f356aed350030bb40eb45ad362c89d9891096f79a3ab323d3ba5607668'), size_on_disk=4976698672, blob_last_accessed=1764885821.7600398, blob_last_modified=1763094406.9482195), CachedFileInfo(file_name='tokenizer.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8'), size_on_disk=9085657, blob_last_accessed=1764885820.0576973, blob_last_modified=1763094180.0307467), CachedFileInfo(file_name='special_tokens_map.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/02ee80b6196926a5ad790a004d9efd6ab1ba6542'), size_on_disk=296, blob_last_accessed=1763094180.3732944, blob_last_modified=1763094180.433744), CachedFileInfo(file_name='tokenizer_config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/db88166e2bc4c799fd5d1ae643b75e84d03ee70e'), size_on_disk=55351, blob_last_accessed=1764885819.8333185, blob_last_modified=1763094179.4065166), CachedFileInfo(file_name='model-00002-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00002-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/09d433f650646834a83c580877bd60c6d1f88f7755305c12576b5c7058f9af15'), size_on_disk=4999802720, blob_last_accessed=1764885826.7408514, blob_last_modified=1763094407.8394845), CachedFileInfo(file_name='model-00004-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00004-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/92ecfe1a2414458b4821ac8c13cf8cb70aed66b5eea8dc5ad9eeb4ff309d6d7b'), size_on_disk=1168138808, blob_last_accessed=1764885834.1224024, blob_last_modified=1763094328.6645076)}), refs=frozenset({'main'}), last_modified=1763094412.2045398)}), last_accessed=1764885835.081519, last_modified=1763094412.2045398), CachedRepoInfo(repo_id='meta-llama/Llama-3.1-8B', repo_type='model', repo_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B'), size_on_disk=16069717568, nb_files=10, revisions=frozenset({CachedRevisionInfo(commit_hash='d04e592bb4f6aa9cfee91e2e20afa771667e1d4b', snapshot_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b'), size_on_disk=16069717568, files=frozenset({CachedFileInfo(file_name='tokenizer_config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/cb9ec25536e44d86778b10509d3e5bdca459a5cf'), size_on_disk=50500, blob_last_accessed=1763075026.8944545, blob_last_modified=1763075026.0800805), CachedFileInfo(file_name='special_tokens_map.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/d8cd5076496dbe4be2320312abc10adc43097b81'), size_on_disk=73, blob_last_accessed=1763075026.7010405, blob_last_modified=1763075026.782478), CachedFileInfo(file_name='model-00004-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model-00004-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/e4486f35c040f683f7d790354f66c169c109eb9fa0954a4a35d7c458a108405d'), size_on_disk=1168138808, blob_last_accessed=1763091693.5002193, blob_last_modified=1763075071.8164258), CachedFileInfo(file_name='tokenizer.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/f916e71031fa08f3c6ef1680a590c15b52d3cdd9'), size_on_disk=9085658, blob_last_accessed=1763075026.904634, blob_last_modified=1763075026.497447), CachedFileInfo(file_name='model-00003-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model-00003-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/d8e9504dd4e4a146d484c52a97584ec14dac92237c46b064934af67a85e7d383'), size_on_disk=4915916176, blob_last_accessed=1763091691.498641, blob_last_modified=1763075146.5609381), CachedFileInfo(file_name='config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/cccf055d6f8f210387a248c91dc40e0c7a4bafab'), size_on_disk=826, blob_last_accessed=1763075180.9273612, blob_last_modified=1763075027.281282), CachedFileInfo(file_name='model-00002-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model-00002-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/c28b25e7541751056ee126627e007f8d4288319733285e9f7b17b9ff6eb313f0'), size_on_disk=4999802720, blob_last_accessed=1763080956.0276797, blob_last_modified=1763075150.8878274), CachedFileInfo(file_name='model.safetensors.index.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/0fd8120f1c6acddc268ebc2583058efaf699a771'), size_on_disk=23950, blob_last_accessed=1763075180.988573, blob_last_modified=1763075027.749547), CachedFileInfo(file_name='model-00001-of-00004.safetensors', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model-00001-of-00004.safetensors'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/f8b9704ab09cdeb097aa4a0a24bca96f906eec36bad63ab495bc21475058601b'), size_on_disk=4976698672, blob_last_accessed=1763075150.9998145, blob_last_modified=1763075150.7147567), CachedFileInfo(file_name='generation_config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/blobs/fc9506438b7b55383dc04c0816561442324846c3'), size_on_disk=185, blob_last_accessed=1763091499.9824295, blob_last_modified=1763091464.6140099)}), refs=frozenset({'main'}), last_modified=1763091464.6140099)}), last_accessed=1763091693.5002193, last_modified=1763091464.6140099), CachedRepoInfo(repo_id='google/pegasus-xsum', repo_type='model', repo_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--google--pegasus-xsum'), size_on_disk=1392, nb_files=1, revisions=frozenset({CachedRevisionInfo(commit_hash='8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b', snapshot_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--google--pegasus-xsum/snapshots/8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b'), size_on_disk=1392, files=frozenset({CachedFileInfo(file_name='config.json', file_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--google--pegasus-xsum/snapshots/8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b/config.json'), blob_path=PosixPath('/home/ceciampa/.cache/huggingface/hub/models--google--pegasus-xsum/blobs/187e4581ab878876269409ed3066a0c45b421d12'), size_on_disk=1392, blob_last_accessed=1762911889.0277314, blob_last_modified=1762911846.5530903)}), refs=frozenset({'main'}), last_modified=1762911846.5530903)}), last_accessed=1762911889.0277314, last_modified=1762911846.5530903)}), warnings=[CorruptedCacheException('Repo path is not a directory: /home/ceciampa/.cache/huggingface/hub/version.txt')])\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "print(scan_cache_dir())\n",
    "# delete_strategy = scan_cache_dir().delete_revisions(\n",
    "#     \"8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b\"\n",
    "# )\n",
    "\n",
    "# print(\"Will free \" + delete_strategy.expected_freed_size_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilModelData:\n",
    "    \"\"\" Class will load data from a tokenizer, model, and a dataset. Also a prompt and labels will be provided.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize the variables\n",
    "        self._train_df = None\n",
    "        self._test_df = None\n",
    "        self._labels = None\n",
    "        self._reversed_labels = None\n",
    "        self._prompt: Callable | None = None\n",
    "        self._num_examples: int = 0\n",
    "        self._model: AutoModelForCausalLM = None\n",
    "        self._tokenizer: AutoTokenizer = None\n",
    "        self._sample = None\n",
    "    \n",
    "    def set_labels(self, labels: Dict[int, str]):\n",
    "        \"\"\"Provided a dictionary of labels it will se the labels. The keys are the integer labels in the dataset and the values of the dictionary are the labels for the prompt into the models.\n",
    "\n",
    "        Args:\n",
    "            labels (Dict[int, str]): The labels to be saved\n",
    "\n",
    "        Raises:\n",
    "            ValueError: A dictionary must be provided as input otherwise an error will be risen.\n",
    "            ValueError: If not all the keys are integers it will cause issues.\n",
    "            ValueError: If not all the values are strings it will raise an error.\n",
    "        \"\"\"\n",
    "        if self._train_df is None or self._test_df is None:\n",
    "            raise ValueError(\"The train and test dataframes have not be set yet. You must set to ensure that each of the labels in the dataframe have been set.\")\n",
    "        if not isinstance(labels, dict):\n",
    "            raise ValueError(\"Labels must be a dictionary\")\n",
    "        if not all(isinstance(k, int) for k in labels.keys()):\n",
    "            raise ValueError(\"Label keys must be integers\")\n",
    "        if not all(isinstance(v, str) for v in labels.values()):\n",
    "            raise ValueError(\"Label values must be strings\")\n",
    "        label_keys = set(labels.keys())\n",
    "        train_df_labels = set(self._train_df['label'].unique())\n",
    "        test_df_labels = set(self._test_df[\"label\"].unique())\n",
    "        if not train_df_labels.issubset(label_keys) or not test_df_labels.issubset(label_keys):\n",
    "            raise ValueError(f\"The provided labels are missing assigned string values for the following values: {', '.join(train_df_labels.difference(label_keys).union(test_df_labels.difference(label_keys)))}.\")\n",
    "        self._labels = labels\n",
    "        self._reversed_labels = {v: k for k, v in self._labels.items()}\n",
    "    \n",
    "    def set_num_examples_in_prompt(self, num: int = 0):\n",
    "        \"\"\"Provided an integer it will set the number of examples in the prompt.\n",
    "\n",
    "        Args:\n",
    "            num (int): The number of examples to be saved.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: An integer must be provided.\n",
    "        \"\"\"\n",
    "        if not isinstance(num, int):\n",
    "            raise ValueError(\"An integer must be provided\")\n",
    "        self._num_examples = num\n",
    "    \n",
    "    def set_prompt(self, prompt_func: Callable[[str, dict, pd.DataFrame], str]):\n",
    "        # Prompt function takes in as such f(string to label, label options, example dataframe) -> prompt string\n",
    "        self._prompt = prompt_func\n",
    "\n",
    "    def set_model(self, model_name: str, bnb_config: None | BitsAndBytesConfig = None):\n",
    "        if not isinstance(model_name, str):\n",
    "            raise ValueError(\"A model name must be provided as a string\")\n",
    "        \n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        print(self._model.device)\n",
    "    \n",
    "    def get_inference_prompt(self, text = \"[FILL IN]\") -> str:\n",
    "        if self._prompt is None:\n",
    "            raise ValueError(\"Prompt has not been set yet.\")\n",
    "        if self._train_df is None:\n",
    "            raise ValueError(\"Train dataset has not been set yet.\")\n",
    "        if self._test_df is None:\n",
    "            raise ValueError(\"Test dataset has not been set yet.\")\n",
    "        if self._labels is None:\n",
    "            raise ValueError(\"Labels have not been set yet.\")\n",
    "        sample = self._sample if self._sample is not None else self._train_df.sample(self._num_examples)\n",
    "        inference_prompt = self._prompt(text, self._labels, sample)\n",
    "        return inference_prompt\n",
    "\n",
    "    def reset_datasets_and_labels(self):\n",
    "        self._labels = None\n",
    "        self._train_df = None\n",
    "        self._test_df = None\n",
    "    \n",
    "    def set_datasets_from_path(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        rename_columns: Dict[str, str] = {},\n",
    "        create_columns: None | Callable[[pd.DataFrame], pd.DataFrame] = None,\n",
    "        ignore_common_text_thresh: float = 0,\n",
    "    ):\n",
    "        # Loads the data\n",
    "        try:\n",
    "            train_temp = pd.read_parquet(train_path)\n",
    "            test_temp = pd.read_parquet(test_path)\n",
    "            # Renames the columns if provided any renames. This is there to help you make sure there is a text and label column as these will be used in this code\n",
    "            train_temp.rename(columns=rename_columns, inplace=True)\n",
    "            test_temp.rename(columns=rename_columns, inplace=True)\n",
    "            # Runs a provided function which modifies the data to ensure that there are columns text and label, and their values are appropriet.\n",
    "            if create_columns is not None:\n",
    "                train_temp = create_columns(train_temp)\n",
    "                test_temp = create_columns(test_temp)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        # This is where it actually sets the data. At this point no errors should have occured so its safe to finally set the values. The last checks will be here.\n",
    "        self.set_datasets(\n",
    "            train_temp.copy(),\n",
    "            test_temp.copy(),\n",
    "            ignore_common_text_thresh=ignore_common_text_thresh,\n",
    "        )\n",
    "    \n",
    "\n",
    "    def set_datasets(self, train_df: pd.DataFrame, test_df: pd.DataFrame, ignore_common_text_thresh: float = 0):\n",
    "        \"\"\"Sets the train and test datasets.\n",
    "\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): The training dataframe.\n",
    "            test_df (pd.DataFrame): The testing dataframe.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Both inputs must be pandas DataFrames.\n",
    "            ValueError: Train DataFrame must have 'text' and 'label' columns.\n",
    "            ValueError: Test DataFrame must have 'text' and 'label' columns.\n",
    "            ValueError: Train DataFrame 'label' column must be of integer type.\n",
    "            ValueError: Test DataFrame 'label' column must be of integer type.\n",
    "            ValueError: Train DataFrame 'text' column must be of string type.\n",
    "            ValueError: Test DataFrame 'text' column must be of string type.\n",
    "            ValueError: Train and Test DataFrames share common text entries. Data leakage detected.\n",
    "        \"\"\"\n",
    "        # Ensures that both of the inputs are DataFrames\n",
    "        if not isinstance(train_df, pd.DataFrame) or not isinstance(test_df, pd.DataFrame):\n",
    "            raise ValueError(\"Both inputs must be pandas DataFrames.\")\n",
    "        \n",
    "        # Checks that there is a labels and text column\n",
    "        if \"text\" not in train_df.columns or \"label\" not in train_df.columns:\n",
    "            raise ValueError(\"Train DataFrame must have 'text' and 'label' columns.\")\n",
    "        if \"text\" not in test_df.columns or \"label\" not in test_df.columns:\n",
    "            raise ValueError(\"Test DataFrame must have 'text' and 'label' columns.\")\n",
    "        \n",
    "        # Ensure that the labels are of the integer type\n",
    "        if not pd.api.types.is_integer_dtype(train_df[\"label\"]):\n",
    "            raise ValueError(\"Train DataFrame 'label' column must be of integer type.\")\n",
    "        if not pd.api.types.is_integer_dtype(test_df[\"label\"]):\n",
    "            raise ValueError(\"Test DataFrame 'label' column must be of integer type.\")\n",
    "        \n",
    "        # Ensure that the text columns are a string value\n",
    "        if not pd.api.types.is_string_dtype(train_df[\"text\"]):\n",
    "            raise ValueError(\"Train DataFrame 'text' column must be of string type\")\n",
    "        if not pd.api.types.is_string_dtype(test_df[\"text\"]):\n",
    "            raise ValueError(\"Test DataFrame 'text' column must be of string type\")\n",
    "        \n",
    "        # Check for overlapping data between train and test sets based on the 'text' column\n",
    "        common_texts = set(train_df[\"text\"]).intersection(set(test_df[\"text\"]))\n",
    "        if common_texts:\n",
    "            perc = len(common_texts) / len(test_df) \n",
    "            err = f\"Data leakage detected! Train and Test DataFrames share {len(common_texts)} ({perc:.2%} of testing dataset) common text entries.\"\n",
    "            if perc > ignore_common_text_thresh:\n",
    "                raise ValueError(err)\n",
    "            else:\n",
    "                warnings.warn(err)\n",
    "        self._train_df = train_df\n",
    "        self._test_df = test_df\n",
    "\n",
    "    def distil_labels(self, batch_size: int = 8, label_prob_prefex: str = 'label_'):\n",
    "        if self._labels is None:\n",
    "            raise ValueError(\"Labels must be set.\")\n",
    "        if self._train_df is None or self._test_df is None:\n",
    "            raise ValueError(\"Datasets must be set.\")\n",
    "        if self._model is None or self._tokenizer is None:\n",
    "            raise ValueError(\"Model and Tokenizer must be set\")\n",
    "        if self._prompt is None:\n",
    "            raise ValueError(\"Prompt must be set.\")\n",
    "        if self._model is None or self._tokenizer is None:\n",
    "            raise ValueError(\"Model and Tokenizer have not been set yet.\")\n",
    "        \n",
    "        # 1. Pre-calculate label token IDs (do this once, not in the loop)\n",
    "        label_token_map = {}\n",
    "        with torch.inference_mode():\n",
    "            for label_str in self._labels.values():\n",
    "                # Add space because many tokenizers are space-sensitive\n",
    "                tokens = self._tokenizer.encode(label_str, add_special_tokens=False)\n",
    "                label_token_map[label_str] = tokens[0]\n",
    "        \n",
    "        self._tokenizer.padding_side = \"left\"\n",
    "        if self._tokenizer.pad_token is None:\n",
    "            self._tokenizer.pad_token = self._tokenizer.eos_token\n",
    "\n",
    "        ordered_labels = list(self._labels.values())\n",
    "        ordered_ids = [label_token_map[l] for l in ordered_labels]\n",
    "        target_token_ids = torch.tensor(ordered_ids, device=self._model.device)\n",
    "        def process_batches(prompts, desc):\n",
    "            \"\"\"Created by Claude Sonnet 4.5\"\"\"\n",
    "            # all_probs = []\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                prompt_lengths = [(i, len(self._tokenizer.encode(p, add_special_tokens=False))) for i, p in enumerate(prompts)]\n",
    "                prompt_lengths.sort(key=lambda x: x[1])\n",
    "\n",
    "                sorted_indices = [x[0] for x in prompt_lengths]\n",
    "                sorted_prompts = [prompts[i] for i in sorted_indices]\n",
    "                batch_results = [None] * len(prompts)\n",
    "                for i in tqdm(range(0, len(sorted_prompts), batch_size), desc=desc):\n",
    "                    batch_prompts = sorted_prompts[i : i + batch_size]\n",
    "                    batch_indices = sorted_indices[i : i + batch_size]\n",
    "\n",
    "                    # Tokenize batch\n",
    "                    inputs = self._tokenizer(\n",
    "                        batch_prompts,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True\n",
    "                    ).to(self._model.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = self._model(**inputs)\n",
    "                    \n",
    "                    # Get logits of last token for each sequence\n",
    "                    # Shape: [batch_size, vocab_size]\n",
    "                    next_token_logits = outputs.logits[:, -1, :]\n",
    "                    \n",
    "                    # Extract logits only for our label tokens\n",
    "                    # Shape: [batch_size, num_labels]\n",
    "                    selected_logits = next_token_logits[:, target_token_ids]\n",
    "                    \n",
    "                    # Softmax over only the selected labels\n",
    "                    probs = F.softmax(selected_logits, dim=-1)\n",
    "                    \n",
    "                    # Convert to list of dicts\n",
    "                    probs_cpu = probs.detach().cpu().numpy()\n",
    "                    for j, row_probs in enumerate(probs_cpu):\n",
    "                        prob_dict = {\n",
    "                            self._reversed_labels[label_str]: p\n",
    "                            for label_str, p in zip(ordered_labels, row_probs)\n",
    "                        }\n",
    "                        batch_results[batch_indices[j]] = prob_dict\n",
    "                    # CRITICAL: Delete GPU tensors explicitly\n",
    "                    del inputs, outputs, next_token_logits, selected_logits, probs, probs_cpu\n",
    "                    \n",
    "                    # Clear CUDA cache periodically (every 10 batches)\n",
    "                    # if i % (batch_size * 10) == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                torch.cuda.empty_cache()\n",
    "                return batch_results\n",
    "        \n",
    "        # Create the prompt for the training\n",
    "        train_prompts = [self._prompt(row[\"text\"], self._labels, self._train_df.drop(i).sample(self._num_examples)) for i, row in self._train_df.iterrows()]\n",
    "        \n",
    "        train_probs = process_batches(train_prompts, \"Getting Probability of Labels Training Dataset\")\n",
    "\n",
    "        train_probs = pd.DataFrame(train_probs).add_prefix(label_prob_prefex)\n",
    "\n",
    "        self._train_df = pd.concat([self._train_df.reset_index(drop=True), train_probs], axis=1)\n",
    "\n",
    "        # Get text exampls for the testing prompts\n",
    "        self._sample = self._train_df.sample(self._num_examples)\n",
    "\n",
    "        # Create test prompts\n",
    "        test_prompts = [self._prompt(row[\"text\"], self._labels, self._sample) for i, row in self._test_df.iterrows()]\n",
    "\n",
    "        # Get the probabilities\n",
    "        test_probs = process_batches(test_prompts, \"Getting Probability of Labels Testing Dataset\")\n",
    "\n",
    "        test_probs = pd.DataFrame(test_probs).add_prefix(label_prob_prefex)\n",
    "\n",
    "        self._test_df = pd.concat([self._test_df.reset_index(drop=True), test_probs], axis=1)\n",
    "\n",
    "    def folder_export(self, path: str):\n",
    "        if self._test_df  is None or self._train_df is None:\n",
    "            raise ValueError(\"The datasets have not been set.\")\n",
    "        self._train_df.to_csv(f\"{path}train.csv\", index=False)\n",
    "        self._test_df.to_csv(f\"{path}test.csv\", index=False)\n",
    "    \n",
    "    def export_files(self, train_path: str, test_path: str):\n",
    "        if self._test_df is None or self._train_df is None:\n",
    "            raise ValueError(\"The datasets have not been set.\")\n",
    "        self._train_df.to_csv(train_path, index=False)\n",
    "        self._test_df.to_csv(test_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34174c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = pd.concat([\n",
    "    pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet\"),\n",
    "    pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/test-00000-of-00001.parquet\")], ignore_index=True).drop_duplicates('text', ignore_index=True)\n",
    "df_total.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1439cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_total, test_size=0.5, random_state=6120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee181c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    12433\n",
      "0    12358\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    12451\n",
      "0    12340\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['label'].value_counts())\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e688d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distallation = DistilModelData()\n",
    "\n",
    "# # \"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])\n",
    "# splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "model_distallation.set_datasets(train_df=df_train, test_df=df_test)\n",
    "\n",
    "model_distallation.set_labels({0: \"Negative\", 1: \"Positive\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e98b09538584ee9ae8600f7db1540a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# model_distallation.set_model(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "model_distallation.set_model(\"dphn/Dolphin3.0-Llama3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e646851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prompt(ex: str, labels: dict, examples: pd.DataFrame) -> str:\n",
    "    output = f\"Classify the sentiment of the following texts as either {', '.join(list(labels.values())[:-1])}, or {list(labels.values())[-1]}.\\n\\n\"\n",
    "    if len(examples) > 0:\n",
    "        for i, row in examples.iterrows():\n",
    "            output += f'Text: {row[\"text\"]}\\nSentiment: {labels[row[\"label\"]]}\\n\\n'\n",
    "    output += f'Text: {ex}\\nSentiment: '\n",
    "    return output\n",
    "\n",
    "model_distallation.set_prompt(model_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e777e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distallation.set_num_examples_in_prompt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9393bdc6f4543f885ec58ac5bcd2316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Probability of Labels Training Dataset:   0%|          | 0/8264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b6405daffc4c9f926ed57b72b4892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Probability of Labels Testing Dataset:   0%|          | 0/8264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_distallation.distil_labels(batch_size=3)\n",
    "NOTIFIER.send_notification(\"The distillation of the labels has been completed successfully.\")\n",
    "print(model_distallation.get_inference_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distallation.folder_export(\"../data/dolphin/\")\n",
    "NOTIFIER.send_notification(\"The export of the distilled data has been completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a084fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The volleyball genre is strangely overlooked b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046021</td>\n",
       "      <td>0.954102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I got interested in this movie because somebod...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.782715</td>\n",
       "      <td>0.217285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sure, I like some indie films. A lot, actually...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955566</td>\n",
       "      <td>0.044525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blademaster is definitely a memorable entry in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069031</td>\n",
       "      <td>0.931152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This World War II Popeye cartoon had some very...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.997070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24786</th>\n",
       "      <td>No reason to bother renting this flick. From t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978516</td>\n",
       "      <td>0.021454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24787</th>\n",
       "      <td>What gives Anthony Minghella the right to ruin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969727</td>\n",
       "      <td>0.030212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24788</th>\n",
       "      <td>My favorite film this year. Great characters a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.993164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24789</th>\n",
       "      <td>Just what is the point of this film? It starts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970703</td>\n",
       "      <td>0.029419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24790</th>\n",
       "      <td>After a cold sex scene, between Andy and Gina,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.998535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24791 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label   label_0  \\\n",
       "0      The volleyball genre is strangely overlooked b...      0  0.046021   \n",
       "1      I got interested in this movie because somebod...      0  0.782715   \n",
       "2      Sure, I like some indie films. A lot, actually...      0  0.955566   \n",
       "3      Blademaster is definitely a memorable entry in...      1  0.069031   \n",
       "4      This World War II Popeye cartoon had some very...      1  0.002823   \n",
       "...                                                  ...    ...       ...   \n",
       "24786  No reason to bother renting this flick. From t...      0  0.978516   \n",
       "24787  What gives Anthony Minghella the right to ruin...      0  0.969727   \n",
       "24788  My favorite film this year. Great characters a...      1  0.006771   \n",
       "24789  Just what is the point of this film? It starts...      0  0.970703   \n",
       "24790  After a cold sex scene, between Andy and Gina,...      1  0.001372   \n",
       "\n",
       "        label_1  \n",
       "0      0.954102  \n",
       "1      0.217285  \n",
       "2      0.044525  \n",
       "3      0.931152  \n",
       "4      0.997070  \n",
       "...         ...  \n",
       "24786  0.021454  \n",
       "24787  0.030212  \n",
       "24788  0.993164  \n",
       "24789  0.029419  \n",
       "24790  0.998535  \n",
       "\n",
       "[24791 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_distallation._train_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
