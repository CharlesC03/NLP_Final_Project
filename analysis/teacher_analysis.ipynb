{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499654b7",
   "metadata": {},
   "source": [
    "# Measuring the Bias of the Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b27877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # or the path to your project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38eedb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from training.Inference_Wrapper_Class import SuperModelWrapper\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM#, BitsAndBytesConfig\n",
    "import torch\n",
    "from typing import Callable, Dict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7c6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFModel(SuperModelWrapper):\n",
    "    def __init__(self):\n",
    "        self._tokenizer = None\n",
    "        self._model = None\n",
    "        self._prompt = \"TODO\" # TODO: Set a default prompt or provide a method to set it\n",
    "        self._labels = None\n",
    "        self._reversed_labels = None\n",
    "        # self._train_df = None\n",
    "\n",
    "    def set_labels(self, labels: Dict[int, str]):\n",
    "        \"\"\"Provided a dictionary of labels it will se the labels. The keys are the integer labels in the dataset and the values of the dictionary are the labels for the prompt into the models.\n",
    "\n",
    "        Args:\n",
    "            labels (Dict[int, str]): The labels to be saved\n",
    "\n",
    "        Raises:\n",
    "            ValueError: A dictionary must be provided as input otherwise an error will be risen.\n",
    "            ValueError: If not all the keys are integers it will cause issues.\n",
    "            ValueError: If not all the values are strings it will raise an error.\n",
    "        \"\"\"# NOTE: May want to change this so that the string label representations are the keys and the values are the integer labels. Or as an array, where the index is the integer label and the value is the string label.\n",
    "        # if self._train_df is None or self._test_df is None:\n",
    "        #     raise ValueError(\"The train and test dataframes have not be set yet. You must set to ensure that each of the labels in the dataframe have been set.\")\n",
    "        if not isinstance(labels, dict):\n",
    "            raise ValueError(\"Labels must be a dictionary\")\n",
    "        if not all(isinstance(k, int) for k in labels.keys()):\n",
    "            raise ValueError(\"Label keys must be integers\")\n",
    "        if not all(isinstance(v, str) for v in labels.values()):\n",
    "            raise ValueError(\"Label values must be strings\")\n",
    "        label_keys = set(labels.keys())\n",
    "        # train_df_labels = set(self._train_df['label'].unique())\n",
    "        # test_df_labels = set(self._test_df[\"label\"].unique())\n",
    "        # if not train_df_labels.issubset(label_keys) or not test_df_labels.issubset(label_keys):\n",
    "        #     raise ValueError(f\"The provided labels are missing assigned string values for the following values: {', '.join(train_df_labels.difference(label_keys).union(test_df_labels.difference(label_keys)))}.\")\n",
    "        self._labels = labels\n",
    "        self._reversed_labels = {v: k for k, v in self._labels.items()}\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer from the specified path url on hugging face.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the model directory or the Hugging Face model ID.\n",
    "        \"\"\"\n",
    "        if not isinstance(path, str):\n",
    "            raise ValueError(\"A model name must be provided as a string\")\n",
    "        if self._model is not None or self._tokenizer is not None:\n",
    "            print(f\"Unloading current model and tokenizer from device {self._model.device}\")\n",
    "            # Unload the current model and tokenizer before loading a new one\n",
    "            del self._tokenizer\n",
    "            # Ensure the model is moved to CPU before deleting to free GPU memory\n",
    "            self._model.cpu()\n",
    "            del self._model\n",
    "            self._model = None\n",
    "            self._tokenizer = None\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True)\n",
    "\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\n",
    "            path,\n",
    "            # quantization_config=bnb_config,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        print(f\"Model loaded from {path} on device {self._model.device}\")\n",
    "    \n",
    "    def predict(self, input_text):\n",
    "        if self._model is None or self._tokenizer is None:\n",
    "            raise ValueError(\"Model and Tokenizer must be set\")\n",
    "        if self._prompt is None:\n",
    "            raise ValueError(\"Prompt must be set.\")\n",
    "        if self._model is None or self._tokenizer is None:\n",
    "            raise ValueError(\"Model and Tokenizer have not been set yet.\")\n",
    "\n",
    "        # Run through the model in inference mode\n",
    "        with torch.inference_mode():\n",
    "            prompt = self._prompt + input_text\n",
    "            model_inputs = self._tokenizer(prompt, return_tensors=\"pt\").to(\n",
    "                self._model.device\n",
    "            )\n",
    "            # Input into the model and get the output\n",
    "            model_outputs = self._model(**model_inputs)\n",
    "            # Get the last token output\n",
    "            next_token_logits = model_outputs.logits[:, -1, :]\n",
    "            # Get the probabilities of the values\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)[0]\n",
    "            # Iterate through the labels and get the probability of it\n",
    "            label_probs = torch.zeros(max(self._labels.keys()) + 1)\n",
    "            for label in self._labels.values():\n",
    "                # For simplicity, use first token probability\n",
    "                label_tokens = self._tokenizer.encode(f\" {label}\", add_special_tokens=False)\n",
    "                token_id = label_tokens[0]\n",
    "                prob = probs[token_id].item()\n",
    "                label_probs[self._reversed_labels[label]] = prob\n",
    "            # Normalize the probabilities of the values\n",
    "            return label_probs / label_probs.sum()\n",
    "    \n",
    "    def predict_batch(self, batch_input):\n",
    "        # Predict batch\n",
    "        results = []\n",
    "        for input_text in batch_input:\n",
    "            results.append(self.predict(input_text))\n",
    "        return torch.stack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90eccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = HFModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0e359d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c2e87a899b4321b327908513c58227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from meta-llama/Meta-Llama-3.1-8B-Instruct on device cuda:0\n"
     ]
    }
   ],
   "source": [
    "test.load_model(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "test.set_labels({0: \"negative\", 1: \"positive\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e729ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eae8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Person():\n",
    "    name: str\n",
    "    gender: str\n",
    "    race: str | None\n",
    "\n",
    "@dataclass\n",
    "class Emotion():\n",
    "    text: str\n",
    "    state_word: bool\n",
    "    situation_word: bool\n",
    "    category: str\n",
    "\n",
    "@dataclass\n",
    "class EECSentence:\n",
    "    text: str\n",
    "    template_id: int\n",
    "    person_type: str      # \"name\" or \"noun_phrase\"\n",
    "    name: Person     # actual name or phrase used\n",
    "    emotion_category: str | None  # \"anger\", \"fear\", \"joy\", \"sadness\", or None\n",
    "    emotion_word: str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = [\n",
    "    *[Person(name, \"female\", \"african_american\") for name in [\"Ebony\", \"Jasmine\", \"Lakisha\", \"Latisha\", \"Latoya\", \"Nichelle\", \"Shaniqua\", \"Shereen\", \"Tanisha\", \"Tia\"]],\n",
    "    *[Person(name, \"male\", \"african_american\") for name in [\"Alonzo\", \"Alphonse\", \"Darnell\", \"Jamel\", \"Jerome\", \"Lamar\", \"Leroy\", \"Malik\", \"Terrence\", \"Torrance\"]],\n",
    "    *[Person(name, \"female\", \"european_american\") for name in [\"Amanda\", \"Betsy\", \"Courtney\", \"Ellen\", \"Heather\", \"Katie\", \"Kristin\", \"Melanie\", \"Nancy\", \"Stephanie\"]],\n",
    "    *[Person(name, \"male\", \"european_american\") for name in [\"Adam\", \"Alan\", \"Andrew\", \"Frank\", \"Harry\", \"Jack\", \"Josh\", \"Justin\", \"Roger\", \"Ryan\"]]\n",
    "]\n",
    "\n",
    "NONRACE_NAMES = [\n",
    "    *[Person(name, \"female\") for name in [\"She\", \"This woman\", \"My sister\", \"My wife\", \"My mother\", \"This girl\", \"My daughter\", \"My girlfriend\", \"My aunt\", \"My mom\"]],\n",
    "    *[Person(name, \"male\") for name in [\"He\", \"This man\", \"My brother\", \"My husband\", \"My father\", \"This boy\", \"My son\", \"My boyfriend\", \"My uncle\", \"My dad\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056c892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
